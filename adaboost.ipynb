{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adaboost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEAdO5yJsYtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "aeb01a81-87d9-4d7f-f52f-6f7f871b415a"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "#data_preprocessing\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from bs4 import BeautifulSoup \n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "!from xgboost import XGBClassifier\n",
        "\n",
        "##import dataset\n",
        "dataset = pd.read_excel('train.xlsx')\n",
        "\n",
        "##Fill missing values of Host with that link in the same row\n",
        "for i in list(dataset[dataset['Host'].isnull() == True]['Host'].index):\n",
        "    dataset.at[i,'Host'] = dataset.loc[i]['Link']\n",
        "    \n",
        "##Filling missing value of TRANS_CONV_TEXT with that of title\n",
        "for i in list(dataset[dataset['TRANS_CONV_TEXT'].isnull() == True].index):\n",
        "    dataset.at[i, 'TRANS_CONV_TEXT'] = dataset.loc[i]['Title']\n",
        "##Converting 'Date(ET)' object to datetime\n",
        "dataset['Date(ET)'] = pd.to_datetime(dataset['Date(ET)'], infer_datetime_format=True)    \n",
        "\n",
        "##Converting Datetime to timestamp\n",
        "dataset['Date(ET)'] = dataset[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n",
        "\n",
        "##Dropping time(ET) and time(GMT)\n",
        "dataset.drop(['Date(ET)', 'Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n",
        "\n",
        "##Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\n",
        "for i in list(dataset[dataset['Title'].isnull() == True]['Title'].index):\n",
        "    dataset.at[i,'Title'] = dataset.loc[i]['TRANS_CONV_TEXT']\n",
        "\n",
        "## TEST Data\n",
        "test = pd.read_csv('test.csv', encoding = 'utf-8')\n",
        "\n",
        "##Fill missing values of Host with that link in the same row\n",
        "for i in list(test[test['Host'].isnull() == True]['Host'].index):\n",
        "    test.at[i,'Host'] = test.loc[i]['Link']\n",
        "    \n",
        "#Filling missing value of TRANS_CONV_TEXT with that of title\n",
        "for i in list(test[test['TRANS_CONV_TEXT'].isnull() == True].index):\n",
        "    test.at[i, 'TRANS_CONV_TEXT'] = test.loc[i]['Title']\n",
        "\n",
        "test.at[441,'Date(ET)'] = test.loc[441, 'Time(ET)']\n",
        "\n",
        "#Converting 'Date(ET)' object to datetime\n",
        "test['Date(ET)'] = pd.to_datetime(test['Date(ET)'], infer_datetime_format=True)    \n",
        "\n",
        "#Converting Datetime to timestamp\n",
        "test['Date(ET)'] = test[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n",
        "\n",
        "#Dropping time(ET) and time(GMT)\n",
        "test.drop(['Date(ET)','Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n",
        "\n",
        "#Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\n",
        "for i in list(test[test['Title'].isnull() == True]['Title'].index):\n",
        "    test.at[i,'Title'] = test.loc[i]['TRANS_CONV_TEXT']\n",
        "\n",
        "index = test['Index']\n",
        "index = list(index)\n",
        "\n",
        "test.drop(['Index'], axis = 1, inplace = True)\n",
        "\n",
        "#feature_selection\n",
        "y = dataset['Patient_Tag'].tolist()\n",
        "\n",
        "dataset['Story'] = 'a'\n",
        "\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    dataset.at[i, 'Story'] = dataset['Source'][i] + ' ' + dataset['Host'][i] + ' ' + str(dataset['Link'][i]) + ' ' + dataset['Title'][i] + ' ' + dataset['TRANS_CONV_TEXT'][i]\n",
        "\n",
        "dataset.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "test['Story'] = 'a'\n",
        "\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test.at[i, 'Story'] = test['Source'][i] + ' ' + test['Host'][i] + ' ' + str(test['Link'][i]) + ' ' + test['Title'][i] + ' ' + test['TRANS_CONV_TEXT'][i]\n",
        "\n",
        "test.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'], axis = 1, inplace = True)\n",
        "\n",
        "replace_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = replace_space.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = bad_symbols.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text \n",
        "\n",
        "dataset['Story'] = dataset['Story'].apply(preprocess)\n",
        "test['Story'] = test['Story'].apply(preprocess)\n",
        "\n",
        "data_train_list = dataset['Story'].tolist()\n",
        "data_test_list = test['Story'].tolist()\n",
        "\n",
        "all_data = data_train_list + data_test_list\n",
        "\n",
        "\n",
        "\n",
        "tt = WordPunctTokenizer()\n",
        "count_vect=CountVectorizer(tokenizer = tt.tokenize ,stop_words ='english', ngram_range = (1,3))\n",
        "count_vect.fit(all_data)\n",
        "train_countvect = count_vect.transform(dataset.Story)\n",
        "test_countvect = count_vect.transform(test.Story)\n",
        "\n",
        "## Feature Selection\n",
        "#Dimensionality reduction using LSI\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(train_countvect)\n",
        "train_countvect_LSI=svd.transform(train_countvect)\n",
        "\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(test_countvect)\n",
        "test_countvect_LSI=svd.transform(test_countvect)\n",
        "\n",
        "#Feature extraction using TFxIDF\n",
        "tf_idf = TfidfVectorizer(min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1.0,smooth_idf=1.0,sublinear_tf=1.0,\n",
        "            stop_words = 'english')\n",
        "tf_idf.fit(all_data)\n",
        "train_tf= tf_idf.transform(dataset.Story)\n",
        "test_tf = tf_idf.transform(test.Story)\n",
        "\n",
        "## Feature Selection\n",
        "#Dimensionality reduction using LSI\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(train_tf)\n",
        "train_tf_LSI=svd.transform(train_tf)\n",
        "\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(test_tf)\n",
        "test_tf_LSI=svd.transform(test_tf)\n",
        "\n",
        "# defining parameter range \n",
        "param_grid = {'n_estimators': [10,50,100,200],\n",
        "     'learning_rate': [0.01, 0.05, 0.1, 1]\n",
        "             }  \n",
        "cv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \n",
        "grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n",
        "\n",
        "# fitting the model for grid search \n",
        "grid.fit(train_tf_LSI, y) \n",
        "\n",
        "# print best parameter after tuning \n",
        "print(grid.best_params_) \n",
        "\n",
        "# print how our model looks after hyper-parameter tuning \n",
        "print(grid.best_estimator_)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "from: can't read /var/mail/xgboost\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 1157 folds for each of 16 candidates, totalling 18512 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    8.0s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:   17.6s\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:   30.5s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:   47.6s\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 1564 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Done 2044 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=-1)]: Done 2588 tasks      | elapsed:  9.0min\n",
            "[Parallel(n_jobs=-1)]: Done 3196 tasks      | elapsed: 14.5min\n",
            "[Parallel(n_jobs=-1)]: Done 3868 tasks      | elapsed: 24.2min\n",
            "[Parallel(n_jobs=-1)]: Done 4604 tasks      | elapsed: 37.5min\n",
            "[Parallel(n_jobs=-1)]: Done 5404 tasks      | elapsed: 38.7min\n",
            "[Parallel(n_jobs=-1)]: Done 6268 tasks      | elapsed: 41.3min\n",
            "[Parallel(n_jobs=-1)]: Done 7196 tasks      | elapsed: 46.7min\n",
            "[Parallel(n_jobs=-1)]: Done 8188 tasks      | elapsed: 56.5min\n",
            "[Parallel(n_jobs=-1)]: Done 9244 tasks      | elapsed: 75.3min\n",
            "[Parallel(n_jobs=-1)]: Done 10364 tasks      | elapsed: 76.6min\n",
            "[Parallel(n_jobs=-1)]: Done 11548 tasks      | elapsed: 81.7min\n",
            "[Parallel(n_jobs=-1)]: Done 12796 tasks      | elapsed: 93.4min\n",
            "[Parallel(n_jobs=-1)]: Done 14108 tasks      | elapsed: 113.1min\n",
            "[Parallel(n_jobs=-1)]: Done 15484 tasks      | elapsed: 116.1min\n",
            "[Parallel(n_jobs=-1)]: Done 16924 tasks      | elapsed: 125.9min\n",
            "[Parallel(n_jobs=-1)]: Done 18428 tasks      | elapsed: 149.3min\n",
            "[Parallel(n_jobs=-1)]: Done 18512 out of 18512 | elapsed: 150.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 1, 'n_estimators': 200}\n",
            "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
            "                   n_estimators=200, random_state=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaEVYgOmsqra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c29b08b8-4c2d-4640-c5c2-a00457264b69"
      },
      "source": [
        "best_params = {'n_estimators': 200, 'learning_rate': 1}\n",
        "ada_clf = AdaBoostClassifier(**best_params)\n",
        "ada_clf.fit(train_tf_LSI, y)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
              "                   n_estimators=200, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DBuVxbn9-bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}