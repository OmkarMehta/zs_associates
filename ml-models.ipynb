{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%reload_ext autoreload\n%autoreload 2\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\n\n#data_preprocessing\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom string import punctuation\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom bs4 import BeautifulSoup \nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve\n\n##import dataset\ndataset = pd.read_excel('train.xlsx')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(dataset[dataset['Host'].isnull() == True]['Host'].index):\n    dataset.at[i,'Host'] = dataset.loc[i]['Link']\n    \n##Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(dataset[dataset['TRANS_CONV_TEXT'].isnull() == True].index):\n    dataset.at[i, 'TRANS_CONV_TEXT'] = dataset.loc[i]['Title']\n##Converting 'Date(ET)' object to datetime\ndataset['Date(ET)'] = pd.to_datetime(dataset['Date(ET)'], infer_datetime_format=True)    \n\n##Converting Datetime to timestamp\ndataset['Date(ET)'] = dataset[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n##Dropping time(ET) and time(GMT)\ndataset.drop(['Date(ET)', 'Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n##Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(dataset[dataset['Title'].isnull() == True]['Title'].index):\n    dataset.at[i,'Title'] = dataset.loc[i]['TRANS_CONV_TEXT']\n\n## TEST Data\ntest = pd.read_csv('test.csv', encoding = 'utf-8')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(test[test['Host'].isnull() == True]['Host'].index):\n    test.at[i,'Host'] = test.loc[i]['Link']\n    \n#Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(test[test['TRANS_CONV_TEXT'].isnull() == True].index):\n    test.at[i, 'TRANS_CONV_TEXT'] = test.loc[i]['Title']\n\ntest.at[441,'Date(ET)'] = test.loc[441, 'Time(ET)']\n\n#Converting 'Date(ET)' object to datetime\ntest['Date(ET)'] = pd.to_datetime(test['Date(ET)'], infer_datetime_format=True)    \n\n#Converting Datetime to timestamp\ntest['Date(ET)'] = test[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n#Dropping time(ET) and time(GMT)\ntest.drop(['Date(ET)','Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n#Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(test[test['Title'].isnull() == True]['Title'].index):\n    test.at[i,'Title'] = test.loc[i]['TRANS_CONV_TEXT']\n\nindex = test['Index']\nindex = list(index)\n\ntest.drop(['Index'], axis = 1, inplace = True)\n\n#feature_selection\ny = dataset['Patient_Tag'].tolist()\n\ndataset['Story'] = 'a'\n\n\nfor i in range(len(dataset)):\n    dataset.at[i, 'Story'] = dataset['Source'][i] + ' ' + dataset['Host'][i] + ' ' + str(dataset['Link'][i]) + ' ' + dataset['Title'][i] + ' ' + dataset['TRANS_CONV_TEXT'][i]\n\ndataset.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Patient_Tag'], axis = 1, inplace = True)\n\n\ntest['Story'] = 'a'\n\n\nfor i in range(len(test)):\n    test.at[i, 'Story'] = test['Source'][i] + ' ' + test['Host'][i] + ' ' + str(test['Link'][i]) + ' ' + test['Title'][i] + ' ' + test['TRANS_CONV_TEXT'][i]\n\ntest.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'], axis = 1, inplace = True)\n\nreplace_space = re.compile('[/(){}\\[\\]\\|@,;]')\nbad_symbols = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess(text):\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = replace_space.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = bad_symbols.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text \n\ndataset['Story'] = dataset['Story'].apply(preprocess)\ntest['Story'] = test['Story'].apply(preprocess)\n\ndata_train_list = dataset['Story'].tolist()\ndata_test_list = test['Story'].tolist()\n\nall_data = data_train_list + data_test_list\n\n\n\ntt = WordPunctTokenizer()\ncount_vect=CountVectorizer(tokenizer = tt.tokenize ,stop_words ='english', ngram_range = (1,3))\ncount_vect.fit(all_data)\ntrain_countvect = count_vect.transform(dataset.Story)\ntest_countvect = count_vect.transform(test.Story)\n\n## Feature Selection\n#Dimensionality reduction using LSI\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(train_countvect)\ntrain_countvect_LSI=svd.transform(train_countvect)\n\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(test_countvect)\ntest_countvect_LSI=svd.transform(test_countvect)\n\n#Feature extraction using TFxIDF\ntf_idf = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1.0,smooth_idf=1.0,sublinear_tf=1.0,\n            stop_words = 'english')\ntf_idf.fit(all_data)\ntrain_tf= tf_idf.transform(dataset.Story)\ntest_tf = tf_idf.transform(test.Story)\n\n## Feature Selection\n#Dimensionality reduction using LSI\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(train_tf)\ntrain_tf_LSI=svd.transform(train_tf)\n\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(test_tf)\ntest_tf_LSI=svd.transform(test_tf)\n\n\n\n\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### SVC"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(SVC(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n'''",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "\"\\n# defining parameter range \\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \\ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(SVC(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_tf_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_) \\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'kernel': 'linear', 'C': 1000}\nsvc_tf = SVC(**best_params, class_weight='balanced', probability=True)\nsvc_tf.fit(train_tf_LSI, y)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n  kernel='linear', max_iter=-1, probability=True, random_state=None,\n  shrinking=True, tol=0.001, verbose=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Logistic Regression for CountVect"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 5, 10,50, 100],\n             'max_iter': [100,200],\n             'multi_class': ['auto', 'ovr'],\n              'penalty': ['l1', 'l2']\n             }  \ncv = KFold(train_countvect_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_countvect_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Fitting 1157 folds for each of 48 candidates, totalling 55536 fits\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    4.6s\n[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:   13.5s\n[Parallel(n_jobs=-1)]: Done 536 tasks      | elapsed:   27.9s\n[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:   47.3s\n[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed:  1.2min\n[Parallel(n_jobs=-1)]: Done 2264 tasks      | elapsed:  1.7min\n[Parallel(n_jobs=-1)]: Done 3096 tasks      | elapsed:  2.3min\n[Parallel(n_jobs=-1)]: Done 4056 tasks      | elapsed:  3.1min\n[Parallel(n_jobs=-1)]: Done 5144 tasks      | elapsed:  4.0min\n[Parallel(n_jobs=-1)]: Done 6360 tasks      | elapsed:  4.9min\n[Parallel(n_jobs=-1)]: Done 7704 tasks      | elapsed:  6.0min\n[Parallel(n_jobs=-1)]: Done 9176 tasks      | elapsed:  7.0min\n[Parallel(n_jobs=-1)]: Done 10776 tasks      | elapsed:  9.4min\n[Parallel(n_jobs=-1)]: Done 12504 tasks      | elapsed: 12.2min\n[Parallel(n_jobs=-1)]: Done 14227 tasks      | elapsed: 15.7min\n[Parallel(n_jobs=-1)]: Done 15219 tasks      | elapsed: 19.1min\n[Parallel(n_jobs=-1)]: Done 16275 tasks      | elapsed: 21.2min\n[Parallel(n_jobs=-1)]: Done 17395 tasks      | elapsed: 25.6min\n[Parallel(n_jobs=-1)]: Done 18579 tasks      | elapsed: 29.0min\n[Parallel(n_jobs=-1)]: Done 19827 tasks      | elapsed: 36.7min\n[Parallel(n_jobs=-1)]: Done 21139 tasks      | elapsed: 42.3min\n[Parallel(n_jobs=-1)]: Done 22515 tasks      | elapsed: 49.9min\n[Parallel(n_jobs=-1)]: Done 23955 tasks      | elapsed: 57.5min\n[Parallel(n_jobs=-1)]: Done 25459 tasks      | elapsed: 63.9min\n[Parallel(n_jobs=-1)]: Done 27027 tasks      | elapsed: 72.7min\n[Parallel(n_jobs=-1)]: Done 28659 tasks      | elapsed: 81.3min\n[Parallel(n_jobs=-1)]: Done 30355 tasks      | elapsed: 89.3min\n[Parallel(n_jobs=-1)]: Done 32115 tasks      | elapsed: 98.4min\n[Parallel(n_jobs=-1)]: Done 33939 tasks      | elapsed: 109.2min\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'C': 0.9, 'max_iter': 100, 'multi_class': 'ovr', 'penalty': 'l2'}\nlr = LogisticRegression(**best_params, class_weight='balanced')\nlr.fit(train_countvect_LSI, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Logistic Regression for TFIDF"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# defining parameter range \nparam_grid = {'C': [1, 5, 10, 100, 250, 500, 1000],\n             'max_iter': [50,100,200],\n             'multi_class': ['auto', 'ovr', 'multinomial'],\n              'penalty': ['l1', 'l2', 'elasticnet']\n             }  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'C': 0.9, 'max_iter': 100, 'multi_class': 'ovr', 'penalty': 'l2'}\nlr = LogisticRegression(**best_params, class_weight='balanced')\nlr_tf.fit(train_tf_LSI, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Multinomial Naive Bayes\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import MultinomialNB\n# defining parameter range \nparam_grid = {'alpha': [0.005, 0.05, 0.1, 0.5, 5, 50]\n             }  \ncv = KFold(train_countvect_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(MultinomialNB(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_countvect_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'alpha': 0.5}\nnb = MultinomialNB(**best_params)\nnb.fit(train_countvect_LSI, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Ensembling"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy.stats.mstats import mode\n\npred1 = svc_tf.predict_proba(test_tf_LSI)\npred2 = lr.predict_proba(test_countvect_LSI)\npred3 = lr_tf.predict_proba(test_tf_LSI)\npred4 = nb.predict_proba(test_countvect_LSI)\n\n\ntest_pred_prob = np.mean([pred1, pred2, pred3, pred4], axis=0)\npred = np.argmax(test_pred_prob, axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame()\nsubmission['Index'] = index\nsubmission['SECTION'] = pred.astype(int)\nsubmission.to_csv(\"ml_prob.csv\", index=None)\nsubmission.head()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}