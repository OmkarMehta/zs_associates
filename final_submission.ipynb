{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install xgboost",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting xgboost\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/24/5fe7237b2eca13ee0cfb100bec8c23f4e69ce9df852a64b0493d49dae4e0/xgboost-0.90-py2.py3-none-manylinux1_x86_64.whl (142.8MB)\n\u001b[K     |████████████████████████████████| 142.8MB 1.2kB/s eta 0:00:013  |▎                               | 1.2MB 4.1MB/s eta 0:00:35     |▌                               | 2.0MB 4.1MB/s eta 0:00:35     |▋                               | 2.7MB 4.1MB/s eta 0:00:35�                               | 3.3MB 4.1MB/s eta 0:00:34     |▉                               | 3.7MB 4.1MB/s eta 0:00:34     |█▌                              | 6.7MB 3.3MB/s eta 0:00:42     |██▎                             | 10.3MB 2.0MB/s eta 0:01:08     |███▏                            | 14.0MB 2.0MB/s eta 0:01:05     |███▏                            | 14.2MB 1.2MB/s eta 0:01:48     |███▋                            | 16.3MB 1.7MB/s eta 0:01:17     |████                            | 18.2MB 751kB/s eta 0:02:46kB/s eta 0:08:14███▎                        | 32.7MB 108kB/s eta 0:16:55███▋                        | 34.0MB 108kB/s eta 0:16:43     |████████▊                       | 38.7MB 101kB/s eta 0:17:05 101kB/s eta 0:17:03     |█████████                       | 39.8MB 101kB/s eta 0:16:55     |█████████▍                      | 41.9MB 130kB/s eta 0:12:56.6MB 1.7MB/s eta 0:00:58.8MB 2.0MB/s eta 0:00:49��█████▎                    | 50.2MB 1.9MB/s eta 0:00:50��█████▋                    | 51.7MB 772kB/s eta 0:01:58��█████▉                    | 53.0MB 2.4MB/s eta 0:00:38��██████                    | 54.0MB 1.3MB/s eta 0:01:08MB/s eta 0:01:08     |████████████▊                   | 57.0MB 175kB/s eta 0:08:09     |█████████████                   | 58.5MB 3.4MB/s eta 0:00:25     |█████████████▏                  | 58.8MB 3.4MB/s eta 0:00:25███▎                  | 59.1MB 3.4MB/s eta 0:00:25█████████▍                  | 59.5MB 3.4MB/s eta 0:00:25     |█████████████▌                  | 60.4MB 3.9MB/s eta 0:00:22     |█████████████▋                  | 60.7MB 3.9MB/s eta 0:00:22     |███████████████▉                | 70.4MB 5.5MB/s eta 0:00:14            | 71.7MB 4.0MB/s eta 0:00:18�███████████▌               | 73.6MB 4.0MB/s eta 0:00:18�███████████▌               | 73.8MB 159kB/s eta 0:07:13�███████████▋               | 74.1MB 159kB/s eta 0:07:10�███████████▉               | 75.3MB 159kB/s eta 0:07:03�████████████               | 75.9MB 153kB/s eta 0:07:15/s eta 0:07:05/s eta 0:07:00/s eta 0:00:41              | 79.4MB 1.6MB/s eta 0:00:41�████████▉              | 79.8MB 1.6MB/s eta 0:00:41��██▎             | 81.5MB 3.4MB/s eta 0:00:19��██▋             | 83.0MB 154kB/s eta 0:06:27��███             | 85.1MB 167kB/s eta 0:05:44█████████▋            | 87.7MB 167kB/s eta 0:05:29  | 88.1MB 1.1MB/s eta 0:00:50███████████████████▉            | 88.4MB 1.1MB/s eta 0:00:50��██████            | 88.8MB 1.1MB/s eta 0:00:49��████████████████            | 89.3MB 181kB/s eta 0:04:56kB/s eta 0:04:25kB/s eta 0:04:24██▊           | 92.4MB 295kB/s eta 0:02:51kB/s eta 0:06:29��█▏          | 94.4MB 133kB/s eta 0:06:04        | 94.7MB 133kB/s eta 0:06:01��███▍          | 95.3MB 216kB/s eta 0:03:40��███▍          | 95.6MB 216kB/s eta 0:03:38��███▉          | 97.7MB 114kB/s eta 0:06:34��████          | 98.2MB 114kB/s eta 0:06:29��██████▎         | 99.3MB 114kB/s eta 0:06:19████████████████▍         | 99.9MB 342kB/s eta 0:02:06��█████████████████████▊         | 101.4MB 542kB/s eta 0:01:17     |███████████████████████         | 102.5MB 107kB/s eta 0:06:16��██████████████████████         | 103.0MB 107kB/s eta 0:06:12 269kB/s eta 0:02:26██▍        | 104.3MB 269kB/s eta 0:02:233�█████████████▌        | 105.0MB 13.9MB/s eta 0:00:03:09:02:08:540MB 188kB/s eta 0:03:10��██████████        | 107.4MB 249kB/s eta 0:02:22██████▍       | 108.9MB 109kB/s eta 0:05:11██████▌       | 109.4MB 109kB/s eta 0:05:07��███▋       | 109.7MB 109kB/s eta 0:05:04�█▋       | 110.0MB 109kB/s eta 0:05:01██████▉       | 110.6MB 108kB/s eta 0:04:57��███▉       | 111.0MB 108kB/s eta 0:04:54�██       | 111.3MB 108kB/s eta 0:04:51��█████████████████▏      | 112.4MB 259kB/s eta 0:01:58��█████████████████▌      | 113.6MB 241kB/s eta 0:02:01114.0MB 241kB/s eta 0:02:00��█████████████████▊      | 114.6MB 115kB/s eta 0:04:05��█████████████████▉      | 115.1MB 115kB/s eta 0:04:01�███████████████████▏     | 116.6MB 193kB/s eta 0:02:16�███████████████████▍     | 117.5MB 193kB/s eta 0:02:11�███████████████████▍     | 117.9MB 193kB/s eta 0:02:09��██████████████████▉     | 119.8MB 79kB/s eta 0:04:51   | 121.9MB 820kB/s eta 0:00:26████████████████████▍    | 122.2MB 820kB/s eta 0:00:26    | 122.6MB 1.3MB/s eta 0:00:16 eta 0:04:22█████████▊    | 123.6MB 198kB/s eta 0:01:37     |████████████████████████████    | 124.8MB 224kB/s eta 0:01:21��████▊   | 128.1MB 62kB/s eta 0:03:55kB/s eta 0:03:50��█████   | 129.1MB 85kB/s eta 0:02:41��█████   | 129.4MB 85kB/s eta 0:02:38��█████   | 129.6MB 85kB/s eta 0:02:36�██████████████▏  | 130.1MB 85kB/s eta 0:02:30�████████████████████████▏  | 130.4MB 85kB/s eta 0:02:26:02:23��██████▍  | 131.0MB 1.4MB/s eta 0:00:09     |█████████████████████████████▍  | 131.1MB 54kB/s eta 0:03:37131.4MB 54kB/s eta 0:03:31�████████████████████████▌  | 131.5MB 54kB/s eta 0:03:28  | 133.5MB 88kB/s eta 0:01:46██████████  | 133.8MB 88kB/s eta 0:01:43|██████████████████████████████▍ | 135.7MB 101kB/s eta 0:01:11    |██████████████████████████████▌ | 135.9MB 101kB/s eta 0:01:09�████████████████████████████▌ | 136.2MB 101kB/s eta 0:01:06B 73kB/s eta 0:00:50B 95kB/s eta 0:00:30███▍| 140.3MB 95kB/s eta 0:00:27��█████████▌| 140.6MB 95kB/s eta 0:00:24�██████████████████████████▊| 141.4MB 95kB/s eta 0:00:162.0MB 517kB/s eta 0:00:02\n\u001b[?25hRequirement already satisfied: numpy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from xgboost) (1.16.2)\nRequirement already satisfied: scipy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from xgboost) (1.1.0)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-0.90\n\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%reload_ext autoreload\n%autoreload 2\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\n\n\n#data_preprocessing\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom string import punctuation\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom bs4 import BeautifulSoup \nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n##import dataset\ndataset = pd.read_excel('train.xlsx')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(dataset[dataset['Host'].isnull() == True]['Host'].index):\n    dataset.at[i,'Host'] = dataset.loc[i]['Link']\n    \n##Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(dataset[dataset['TRANS_CONV_TEXT'].isnull() == True].index):\n    dataset.at[i, 'TRANS_CONV_TEXT'] = dataset.loc[i]['Title']\n##Converting 'Date(ET)' object to datetime\ndataset['Date(ET)'] = pd.to_datetime(dataset['Date(ET)'], infer_datetime_format=True)    \n\n##Converting Datetime to timestamp\ndataset['Date(ET)'] = dataset[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n##Dropping time(ET) and time(GMT)\ndataset.drop(['Date(ET)', 'Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n##Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(dataset[dataset['Title'].isnull() == True]['Title'].index):\n    dataset.at[i,'Title'] = dataset.loc[i]['TRANS_CONV_TEXT']\n\n## TEST Data\ntest = pd.read_csv('test.csv', encoding = 'utf-8')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(test[test['Host'].isnull() == True]['Host'].index):\n    test.at[i,'Host'] = test.loc[i]['Link']\n    \n#Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(test[test['TRANS_CONV_TEXT'].isnull() == True].index):\n    test.at[i, 'TRANS_CONV_TEXT'] = test.loc[i]['Title']\n\ntest.at[441,'Date(ET)'] = test.loc[441, 'Time(ET)']\n\n#Converting 'Date(ET)' object to datetime\ntest['Date(ET)'] = pd.to_datetime(test['Date(ET)'], infer_datetime_format=True)    \n\n#Converting Datetime to timestamp\ntest['Date(ET)'] = test[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n#Dropping time(ET) and time(GMT)\ntest.drop(['Date(ET)','Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n#Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(test[test['Title'].isnull() == True]['Title'].index):\n    test.at[i,'Title'] = test.loc[i]['TRANS_CONV_TEXT']\n\nindex = test['Index']\nindex = list(index)\n\ntest.drop(['Index'], axis = 1, inplace = True)\n\n#feature_selection\ny = dataset['Patient_Tag'].tolist()\n\ndataset['Story'] = 'a'\n\n\nfor i in range(len(dataset)):\n    dataset.at[i, 'Story'] = dataset['Source'][i] + ' ' + dataset['Host'][i] + ' ' + str(dataset['Link'][i]) + ' ' + dataset['Title'][i] + ' ' + dataset['TRANS_CONV_TEXT'][i]\n\ndataset.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT'], axis = 1, inplace = True)\n\n\ntest['Story'] = 'a'\n\n\nfor i in range(len(test)):\n    test.at[i, 'Story'] = test['Source'][i] + ' ' + test['Host'][i] + ' ' + str(test['Link'][i]) + ' ' + test['Title'][i] + ' ' + test['TRANS_CONV_TEXT'][i]\n\ntest.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'], axis = 1, inplace = True)\n\nreplace_space = re.compile('[/(){}\\[\\]\\|@,;]')\nbad_symbols = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess(text):\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = replace_space.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = bad_symbols.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text \n\ndataset['Story'] = dataset['Story'].apply(preprocess)\ntest['Story'] = test['Story'].apply(preprocess)\n\ndata_train_list = dataset['Story'].tolist()\ndata_test_list = test['Story'].tolist()\n\nall_data = data_train_list + data_test_list\n\n\n\ntt = WordPunctTokenizer()\ncount_vect=CountVectorizer(tokenizer = tt.tokenize ,stop_words ='english', ngram_range = (1,3))\ncount_vect.fit(all_data)\ntrain_countvect = count_vect.transform(dataset.Story)\ntest_countvect = count_vect.transform(test.Story)\n\n## Feature Selection\n#Dimensionality reduction using LSI\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(train_countvect)\ntrain_countvect_LSI=svd.transform(train_countvect)\n\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(test_countvect)\ntest_countvect_LSI=svd.transform(test_countvect)\n\n#Feature extraction using TFxIDF\ntf_idf = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1.0,smooth_idf=1.0,sublinear_tf=1.0,\n            stop_words = 'english')\ntf_idf.fit(all_data)\ntrain_tf= tf_idf.transform(dataset.Story)\ntest_tf = tf_idf.transform(test.Story)\n\n## Feature Selection\n#Dimensionality reduction using LSI\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(train_tf)\ntrain_tf_LSI=svd.transform(train_tf)\n\nsvd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\nsvd.fit(test_tf)\ntest_tf_LSI=svd.transform(test_tf)\n\n\n\n\n",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Adaboost"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n# defining parameter range \nparam_grid = {'n_estimators': np.arange(10,300,25),\n     'learning_rate': [0.01, 0.05, 0.1, 1]\n             }  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(AdaBoostClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_)\n'''",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "\"\\n# defining parameter range \\nparam_grid = {'n_estimators': np.arange(10,300,25),\\n     'learning_rate': [0.01, 0.05, 0.1, 1]\\n             }  \\ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(AdaBoostClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_tf_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_)\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'n_estimators': 200 , 'learning_rate': 1 }\nada_clf = AdaBoostClassifier(**best_params)\nada_clf.fit(train_tf_LSI, y)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n          n_estimators=200, random_state=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### GradientBoosting"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n# defining parameter range \nparam_grid = {'n_estimators': [10,25,50,75,100],\n     'learning_rate': [0.01, 0.05, 0.1, 1]\n             }  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_)\n'''",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "\"\\n# defining parameter range \\nparam_grid = {'n_estimators': [10,25,50,75,100],\\n     'learning_rate': [0.01, 0.05, 0.1, 1],\\n              'max_depth' : np.arange(1,10)\\n             }  \\ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(GradientBoostingClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_tf_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_)\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'n_estimators': 200, 'learning_rate': 1}\ngrad_clf = GradientBoostingClassifier(**best_params)\ngrad_clf.fit(train_tf_LSI, y)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=1, loss='deviance', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=200,\n              n_iter_no_change=None, presort='auto', random_state=None,\n              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### XGBoost\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n# defining parameter range \nparam_grid = {'reg_alpha': np.arange(0,5),\n              'reg_lambda': np.arange(0,5),\n     'learning_rate': [0.01, 0.05, 0.1, 1],\n              'max_depth' = np.arange(1,10)\n             }  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(XBGClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_)\n'''",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "\"\\n# defining parameter range \\nparam_grid = {'reg_alpha': np.arange(0,5),\\n              'reg_lambda': np.arange(0,5),\\n     'learning_rate': [0.01, 0.05, 0.1, 1],\\n              'max_depth' = np.arange(1,10)\\n             }  \\ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(XBGClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_tf_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_)\\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nbest_params = {'reg_alpha': 1, 'reg_lambda': 1, 'learning_rate': 1}\nxgb_clf = XGBClassifier(**best_params)\nxgb_clf.fit(train_tf_LSI, y)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=1,\n       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None,\n       objective='binary:logistic', random_state=0, reg_alpha=1,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n       subsample=1, verbosity=1)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### RandomForestClassifier"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# defining parameter range \nparam_grid = {'n_estimators': [250, 500, 750, 1000, 1500, 2000],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': [25, 50, 75, 100]\n             }  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(RandomForestClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'n_estimators': ,\n               'max_features': ,\n               'max_depth': }\nrf_clf = RandomForestClassifier(**best_params)\nrf_clf.fit(train_tf_LSI, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### SVC"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(SVC(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n'''",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "\"\\n# defining parameter range \\nparam_grid = {'C': [0.1, 1, 10, 100, 1000],\\n             'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}  \\ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(SVC(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_tf_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_) \\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'kernel': 'linear', 'C': 1000}\nsvc_tf = SVC(**best_params, class_weight='balanced', probability=True)\nsvc_tf.fit(train_tf_LSI, y)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "SVC(C=1000, cache_size=200, class_weight='balanced', coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n  kernel='linear', max_iter=-1, probability=True, random_state=None,\n  shrinking=True, tol=0.001, verbose=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Logistic Regression for CountVect"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\nfrom sklearn.linear_model import LogisticRegression\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 5, 10,50, 100],\n             'max_iter': [100,200],\n             'multi_class': ['auto', 'ovr'],\n              'penalty': ['l1', 'l2']\n             }  \ncv = KFold(train_countvect_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_countvect_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n'''",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "\"\\nfrom sklearn.linear_model import LogisticRegression\\n# defining parameter range \\nparam_grid = {'C': [0.1, 1, 5, 10,50, 100],\\n             'max_iter': [100,200],\\n             'multi_class': ['auto', 'ovr'],\\n              'penalty': ['l1', 'l2']\\n             }  \\ncv = KFold(train_countvect_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_countvect_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_) \\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'C': 1,'max_iter': 100, 'multi_class': 'auto','penalty': 'l1'}\nlr = LogisticRegression(**best_params, class_weight='balanced')\nlr.fit(train_countvect_LSI, y)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "LogisticRegression(C=1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='auto', n_jobs=None, penalty='l1', random_state=None,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Logistic Regression for TFIDF"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\n# defining parameter range \nparam_grid = {'C': [1, 5, 10, 100, 250, 500, 1000],\n             'max_iter': [50,100,200],\n             'multi_class': ['auto', 'ovr', 'multinomial'],\n              'penalty': ['l1', 'l2', 'elasticnet']\n             }  \ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n\n# fitting the model for grid search \ngrid.fit(train_tf_LSI, y) \n\n# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n'''",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "\"\\n# defining parameter range \\nparam_grid = {'C': [1, 5, 10, 100, 250, 500, 1000],\\n             'max_iter': [50,100,200],\\n             'multi_class': ['auto', 'ovr', 'multinomial'],\\n              'penalty': ['l1', 'l2', 'elasticnet']\\n             }  \\ncv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \\n\\n# fitting the model for grid search \\ngrid.fit(train_tf_LSI, y) \\n\\n# print best parameter after tuning \\nprint(grid.best_params_) \\n\\n# print how our model looks after hyper-parameter tuning \\nprint(grid.best_estimator_) \\n\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_params = {'C': 250, 'max_iter': 100, 'penalty': 'l2'}\nlr_tf = LogisticRegression(**best_params, class_weight='balanced')\nlr_tf.fit(train_tf_LSI, y)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "LogisticRegression(C=250, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Ensembling"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from scipy.stats.mstats import mode\n\npred1 = ada_clf.predict_proba(test_tf_LSI)\npred2 = grad_clf.predict_proba(test_tf_LSI)\npred3 = xgb_clf.predict_proba(test_tf_LSI)\npred4 = rf_clf.predict_proba(test_tf_LSI)\npred5 = svc_tf.predict_proba(test_tf_LSI)\npred6 = lr.predict_proba(test_countvect_LSI)\npred7 = lr_tf.predict_proba(test_tf_LSI)\n\n\n\ntest_pred_prob = np.mean([pred1, pred2, pred3, pred4, pred5, pred6, pred7], axis=0)\npred = np.argmax(test_pred_prob, axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame()\nsubmission['Index'] = index\nsubmission['SECTION'] = pred.astype(int)\nsubmission.to_csv(\"ml_prob.csv\", index=None)\nsubmission.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### LSTM"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%reload_ext autoreload\n%autoreload 2\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport string\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n\n## Plot\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport matplotlib as plt\n\n# NLTK\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\n# Other\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.manifold import TSNE\n#data_preprocessing\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom string import punctuation\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom bs4 import BeautifulSoup \nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n#from xgboost import XGBClassifier\n\n##import dataset\ndataset = pd.read_excel('train.xlsx')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(dataset[dataset['Host'].isnull() == True]['Host'].index):\n    dataset.at[i,'Host'] = dataset.loc[i]['Link']\n    \n##Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(dataset[dataset['TRANS_CONV_TEXT'].isnull() == True].index):\n    dataset.at[i, 'TRANS_CONV_TEXT'] = dataset.loc[i]['Title']\n##Converting 'Date(ET)' object to datetime\ndataset['Date(ET)'] = pd.to_datetime(dataset['Date(ET)'], infer_datetime_format=True)    \n\n##Converting Datetime to timestamp\ndataset['Date(ET)'] = dataset[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n##Dropping time(ET) and time(GMT)\ndataset.drop(['Date(ET)', 'Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n##Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(dataset[dataset['Title'].isnull() == True]['Title'].index):\n    dataset.at[i,'Title'] = dataset.loc[i]['TRANS_CONV_TEXT']\n\n## TEST Data\ntest = pd.read_csv('test.csv', encoding = 'utf-8')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(test[test['Host'].isnull() == True]['Host'].index):\n    test.at[i,'Host'] = test.loc[i]['Link']\n    \n#Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(test[test['TRANS_CONV_TEXT'].isnull() == True].index):\n    test.at[i, 'TRANS_CONV_TEXT'] = test.loc[i]['Title']\n\ntest.at[441,'Date(ET)'] = test.loc[441, 'Time(ET)']\n\n#Converting 'Date(ET)' object to datetime\ntest['Date(ET)'] = pd.to_datetime(test['Date(ET)'], infer_datetime_format=True)    \n\n#Converting Datetime to timestamp\ntest['Date(ET)'] = test[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n#Dropping time(ET) and time(GMT)\ntest.drop(['Date(ET)','Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n#Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(test[test['Title'].isnull() == True]['Title'].index):\n    test.at[i,'Title'] = test.loc[i]['TRANS_CONV_TEXT']\n\nindex = test['Index']\nindex = list(index)\n\ntest.drop(['Index'], axis = 1, inplace = True)\n\n#feature_selection\ny = dataset['Patient_Tag'].tolist()\n\ndataset['Story'] = 'a'\n\n\nfor i in range(len(dataset)):\n    dataset.at[i, 'Story'] = dataset['Source'][i] + ' ' + dataset['Host'][i] + ' ' + str(dataset['Link'][i]) + ' ' + dataset['Title'][i] + ' ' + dataset['TRANS_CONV_TEXT'][i]\n\ndataset.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT'], axis = 1, inplace = True)\n\n\ntest['Story'] = 'a'\n\n\nfor i in range(len(test)):\n    test.at[i, 'Story'] = test['Source'][i] + ' ' + test['Host'][i] + ' ' + str(test['Link'][i]) + ' ' + test['Title'][i] + ' ' + test['TRANS_CONV_TEXT'][i]\n\ntest.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'], axis = 1, inplace = True)\n\nreplace_space = re.compile('[/(){}\\[\\]\\|@,;]')\nbad_symbols = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess(text):\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text \n\ndataset['Story'] = dataset['Story'].map(lambda x: preprocess(x))\ntest['Story'] = test['Story'].map(lambda x: preprocess(x))\n\ndata_train_list = dataset['Story'].tolist()\ndata_test_list = test['Story'].tolist()\n\nall_data = data_train_list + data_test_list\n\n\n\n\n#Part 1\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n## Plotly\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n# Others\nimport string\n\nfrom sklearn.manifold import TSNE\n\n\n### Create sequence\nvocabulary_size = 20000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(dataset['Story'])\nsequences = tokenizer.texts_to_sequences(dataset['Story'])\ndata = pad_sequences(sequences, maxlen=50)\n\n#test\nsequences_test = tokenizer.texts_to_sequences(test['Story'])\nx_test = pad_sequences(sequences_test, maxlen = 50)\n\n\n## Network architecture\nmodel = Sequential()\nmodel.add(Embedding(20000, 100, input_length=50))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n## Fit the model\nmodel.fit(data, dataset['Patient_Tag'], validation_split=0.3, epochs=25)\n\n\n\ny_pred = model.predict([x_test], batch_size=32, verbose=0)\ny_test = []\nfor i in range(len(y_pred)):\n  if y_pred[i][0] > 0.7:\n    y_test.append(1)\n  else:\n    y_test.append(0)\n\nsubmission = pd.DataFrame()\nsubmission['Index'] = index\nsubmission['Patient_Tag'] = y_test\nsubmission.to_csv(\"lstm_.csv\", index=None)\nsubmission.head()\n\n\n\n# Part 2: LSTM with CNN\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n## Plotly\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n# Others\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\n\nfrom sklearn.manifold import TSNE\n\n\ndef create_conv_model():\n    model_conv = Sequential()\n    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n    model_conv.add(Dropout(0.2))\n    model_conv.add(Conv1D(64, 5, activation='relu'))\n    model_conv.add(MaxPooling1D(pool_size=4))\n    model_conv.add(LSTM(100))\n    model_conv.add(Dense(1, activation='sigmoid'))\n    model_conv.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n    return model_conv\n\nmodel_conv = create_conv_model()\nmodel_conv.fit(data, dataset['Patient_Tag'], validation_split=0.3, epochs = 15)\n\n\n\ny_pred = model_conv.predict([x_test], batch_size=32, verbose=0)\ny_test = []\nfor i in range(len(y_pred)):\n  if y_pred[i][0] > 0.7:\n    y_test.append(1)\n  else:\n    y_test.append(0)\n\nsubmission = pd.DataFrame()\nsubmission['Index'] = index\nsubmission['Patient_Tag'] = y_test\nsubmission.to_csv(\"lstm_cnn.csv\", index=None)\nsubmission.head()\n\n\n\n# Part 3: LSTM+CNN+Glove Embedding\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n## Plotly\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n# Others\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom sklearn.manifold import TSNE\n\n# Extract word embeddings from Glove\nembeddings_index = dict()\nf = open('glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\n\n# Create a weight matrix\nembedding_matrix = np.zeros((vocabulary_size, 50))\nfor word, index_ in tokenizer.word_index.items():\n    if index_ > vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index_] = embedding_vector\n\n## create model\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(vocabulary_size, 50, input_length=50, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Conv1D(64, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\nmodel_glove.add(LSTM(100))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n## Fit train data\nmodel_glove.fit(data, dataset['Patient_Tag'], validation_split=0.4, epochs = 25)\n\n\ny_pred = model_glove.predict([x_test], batch_size=32, verbose=0)\ny_test = []\nfor i in range(len(y_pred)):\n  if y_pred[i][0] > 0.7:\n    y_test.append(1)\n  else:\n    y_test.append(0)\n\nsubmission = pd.DataFrame()\nsubmission['Index'] = index\nsubmission['Patient_Tag'] = y_test\nsubmission.to_csv(\"lstm_cnn_glove.csv\", index=None)\nsubmission.head()\n\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Fastai"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install fastai\n\n%reload_ext autoreload\n%autoreload 2\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n#data_preprocessing\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import randint as sp_randint\nfrom time import time\n\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction import text\nfrom nltk.tokenize import TweetTokenizer\n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom string import punctuation\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom bs4 import BeautifulSoup \nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_curve\n\n##import dataset\ndataset = pd.read_excel('train.xlsx')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(dataset[dataset['Host'].isnull() == True]['Host'].index):\n    dataset.at[i,'Host'] = dataset.loc[i]['Link']\n    \n##Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(dataset[dataset['TRANS_CONV_TEXT'].isnull() == True].index):\n    dataset.at[i, 'TRANS_CONV_TEXT'] = dataset.loc[i]['Title']\n##Converting 'Date(ET)' object to datetime\ndataset['Date(ET)'] = pd.to_datetime(dataset['Date(ET)'], infer_datetime_format=True)    \n\n##Converting Datetime to timestamp\ndataset['Date(ET)'] = dataset[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n##Dropping time(ET) and time(GMT)\ndataset.drop(['Date(ET)', 'Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n##Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(dataset[dataset['Title'].isnull() == True]['Title'].index):\n    dataset.at[i,'Title'] = dataset.loc[i]['TRANS_CONV_TEXT']\n\n## TEST Data\ntest = pd.read_csv('test.csv', encoding = 'utf-8')\n\n##Fill missing values of Host with that link in the same row\nfor i in list(test[test['Host'].isnull() == True]['Host'].index):\n    test.at[i,'Host'] = test.loc[i]['Link']\n    \n#Filling missing value of TRANS_CONV_TEXT with that of title\nfor i in list(test[test['TRANS_CONV_TEXT'].isnull() == True].index):\n    test.at[i, 'TRANS_CONV_TEXT'] = test.loc[i]['Title']\n\ntest.at[441,'Date(ET)'] = test.loc[441, 'Time(ET)']\n\n#Converting 'Date(ET)' object to datetime\ntest['Date(ET)'] = pd.to_datetime(test['Date(ET)'], infer_datetime_format=True)    \n\n#Converting Datetime to timestamp\ntest['Date(ET)'] = test[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n\n#Dropping time(ET) and time(GMT)\ntest.drop(['Date(ET)','Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n\n#Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\nfor i in list(test[test['Title'].isnull() == True]['Title'].index):\n    test.at[i,'Title'] = test.loc[i]['TRANS_CONV_TEXT']\n\nindex = test['Index']\nindex = list(index)\n\ntest.drop(['Index'], axis = 1, inplace = True)\n\n#feature_selection\ny = dataset['Patient_Tag'].tolist()\n\ndataset['Story'] = 'a'\n\n\nfor i in range(len(dataset)):\n    dataset.at[i, 'Story'] = dataset['Source'][i] + ' ' + dataset['Host'][i] + ' ' + str(dataset['Link'][i]) + ' ' + dataset['Title'][i] + ' ' + dataset['TRANS_CONV_TEXT'][i]\n\ndataset.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT'], axis = 1, inplace = True)\n\n\ntest['Story'] = 'a'\n\n\nfor i in range(len(test)):\n    test.at[i, 'Story'] = test['Source'][i] + ' ' + test['Host'][i] + ' ' + str(test['Link'][i]) + ' ' + test['Title'][i] + ' ' + test['TRANS_CONV_TEXT'][i]\n\ntest.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'], axis = 1, inplace = True)\n\nreplace_space = re.compile('[/(){}\\[\\]\\|@,;]')\nbad_symbols = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprocess(text):\n    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = replace_space.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = bad_symbols.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text \n\ndataset['Story'] = dataset['Story'].apply(preprocess)\ntest['Story'] = test['Story'].apply(preprocess)\n\ndata_train_list = dataset['Story'].tolist()\ndata_test_list = test['Story'].tolist()\n\nall_data = data_train_list + data_test_list\n\nprint(\"size of test data as compared to train data {:0.2f}%\".format(len(test)/len(dataset)*100))\n\n#Returning average word length of phrases for each dataset\nprint('Average word length of phrases in train is {0:.0f}.'.format(np.mean(dataset['Story'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Story'].apply(lambda x: len(x.split())))))\n\n#Graph\nfig, ax = plt.subplots(1,1, dpi = 100, figsize = (10,5))\ndata_ = pd.read_excel('train.xlsx')\npatient_tags_labels = data_['Patient_Tag'].value_counts().index\npatient_tags_count = data_['Patient_Tag'].value_counts()\n\n#Plotting\nsns.barplot(x = patient_tags_labels, y = patient_tags_count)\n\nax.set_ylabel('Count')\nax.set_xlabel('Patient Tag Label')\nax.set_xticklabels(patient_tags_labels)\n\n\nfrom fastai.text import *\n\ndata = (TextList.from_df(dataset, cols = 'Story').split_by_rand_pct(0.2).label_for_lm().databunch(bs = 48))\ndata.show_batch()\n\nlearn = language_model_learner(data, AWD_LSTM, drop_mult = 0.3)\nlearn.fit_one_cycle(4)\nlearn.save('stage-1')\nlearn.unfreeze()\nlearn.fit_one_cycle(1)\nlearn.load('stage-1')\n\nlearn.lr_find()\nlearn.recorder.plot()\n\nlearn.unfreeze()\nlearn.fit_one_cycle(3)\nlearn.save_encoder('fine_tuned_enc')\nlearn.load_encoder('fine_tuned_enc')\n\ntest_datalist = TextList.from_df(test, cols = 'Story', vocab = data.vocab)\n\ndata_clas = (TextList.from_df(dataset, cols = 'Story', vocab = data.vocab).split_by_rand_pct(0.2)\n            .label_from_df(cols = 'Patient_Tag')\n            .add_test(test_datalist)\n            .databunch(bs = 32))\n\ndata_clas.show_batch()\n\nlearn_classifier = text_classifier_learner(data_clas, AWD_LSTM, drop_mult = 0.3)\nlearn_classifier.load_encoder('fine_tuned_enc')\nlearn_classifier.freeze()\n\nlearn_classifier.lr_find()\nlearn_classifier.recorder.plot()\nlearn_classifier.fit_one_cycle(1, 2e-2, moms = (0.8, 0.7))\nlearn_classifier.save('first')\nlearn_classifier.load('first')\n\nlearn_classifier.freeze_to(-2)\nlearn_classifier.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2), moms = (0.8, 0.7))\nlearn_classifier.save('second')\nlearn_classifier.load('second')\n\nlearn_classifier.freeze_to(-3)\nlearn_classifier.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3), moms = (0.8, 0.7))\n\nlearn_classifier.save('third')\nlearn_classifier.load('third')\n\nlearn_classifier.unfreeze()\nlearn_classifier.fit_one_cycle(3, slice(1e-3/(2.6**4), 1e-3), moms = (0.8,0.7))\n\npreds, target = learn_classifier.get_preds(DatasetType.Test, ordered = True)\nlabels = np.argmax(preds, axis = 1)\n\nlearn_classifier.show_results()\n\nnp.save(\"fastai_prob.npy\", preds)\n\nsubmission = pd.DataFrame()\nsubmission['Index'] = index\nsubmission['Patient_Tag'] = labels\nsubmission.to_csv(\"fastai_model.csv\", index=None)\nsubmission.head()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Final Ensemble"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import mode\n\npred1 = pd.read_csv(\"ml_prob.csv\", index_col=None)\npred2 = np.argmax(np.load('fastai_prob.npy'), axis=1)\npred3 = pd.read_csv(\"lstm_.csv\", index_col=None)\npred4 = pd.read_csv(\"lstm_cnn.csv\", index_col=None)\npred5 = pd.read_csv(\"lstm_cnn_glove.csv\", index_col=None)\n\n\nfinal_ensemble = mode((pred1['Patient_Tag'], pred2, pred3['Patient_Tag'], pred4['Patient_Tag'], pred5['Patient_Tag']))[0][0]\nsubmission = pd.DataFrame()\nsubmission['Index'] = pred1['Index']\nsubmission['Patient_Tag'] = final_ensemble\nsubmission.to_csv('ensemble_final.csv', index=None)\nprint(submission['Patient_Tag'].value_counts())\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}