{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xgboosting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LZdYO4kvPfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c6214fde-e8f8-46d9-a444-8829af3198be"
      },
      "source": [
        "!pip install xgboost"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N09fLL9qtrS4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "b884d38e-270e-48bb-bfee-6bac6bd482e8"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "#data_preprocessing\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from bs4 import BeautifulSoup \n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "##import dataset\n",
        "dataset = pd.read_excel('train.xlsx')\n",
        "\n",
        "##Fill missing values of Host with that link in the same row\n",
        "for i in list(dataset[dataset['Host'].isnull() == True]['Host'].index):\n",
        "    dataset.at[i,'Host'] = dataset.loc[i]['Link']\n",
        "    \n",
        "##Filling missing value of TRANS_CONV_TEXT with that of title\n",
        "for i in list(dataset[dataset['TRANS_CONV_TEXT'].isnull() == True].index):\n",
        "    dataset.at[i, 'TRANS_CONV_TEXT'] = dataset.loc[i]['Title']\n",
        "##Converting 'Date(ET)' object to datetime\n",
        "dataset['Date(ET)'] = pd.to_datetime(dataset['Date(ET)'], infer_datetime_format=True)    \n",
        "\n",
        "##Converting Datetime to timestamp\n",
        "dataset['Date(ET)'] = dataset[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n",
        "\n",
        "##Dropping time(ET) and time(GMT)\n",
        "dataset.drop(['Date(ET)', 'Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n",
        "\n",
        "##Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\n",
        "for i in list(dataset[dataset['Title'].isnull() == True]['Title'].index):\n",
        "    dataset.at[i,'Title'] = dataset.loc[i]['TRANS_CONV_TEXT']\n",
        "\n",
        "## TEST Data\n",
        "test = pd.read_csv('test.csv', encoding = 'utf-8')\n",
        "\n",
        "##Fill missing values of Host with that link in the same row\n",
        "for i in list(test[test['Host'].isnull() == True]['Host'].index):\n",
        "    test.at[i,'Host'] = test.loc[i]['Link']\n",
        "    \n",
        "#Filling missing value of TRANS_CONV_TEXT with that of title\n",
        "for i in list(test[test['TRANS_CONV_TEXT'].isnull() == True].index):\n",
        "    test.at[i, 'TRANS_CONV_TEXT'] = test.loc[i]['Title']\n",
        "\n",
        "test.at[441,'Date(ET)'] = test.loc[441, 'Time(ET)']\n",
        "\n",
        "#Converting 'Date(ET)' object to datetime\n",
        "test['Date(ET)'] = pd.to_datetime(test['Date(ET)'], infer_datetime_format=True)    \n",
        "\n",
        "#Converting Datetime to timestamp\n",
        "test['Date(ET)'] = test[['Date(ET)']].apply(lambda x: x[0].timestamp(), axis = 1).astype(int)\n",
        "\n",
        "#Dropping time(ET) and time(GMT)\n",
        "test.drop(['Date(ET)','Time(ET)', 'time(GMT)'], axis = 1, inplace = True)\n",
        "\n",
        "#Fill missing values of Title with that of TRANS_CONV_TEXT in the same row\n",
        "for i in list(test[test['Title'].isnull() == True]['Title'].index):\n",
        "    test.at[i,'Title'] = test.loc[i]['TRANS_CONV_TEXT']\n",
        "\n",
        "index = test['Index']\n",
        "index = list(index)\n",
        "\n",
        "test.drop(['Index'], axis = 1, inplace = True)\n",
        "\n",
        "#feature_selection\n",
        "y = dataset['Patient_Tag'].tolist()\n",
        "\n",
        "dataset['Story'] = 'a'\n",
        "\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    dataset.at[i, 'Story'] = dataset['Source'][i] + ' ' + dataset['Host'][i] + ' ' + str(dataset['Link'][i]) + ' ' + dataset['Title'][i] + ' ' + dataset['TRANS_CONV_TEXT'][i]\n",
        "\n",
        "dataset.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "test['Story'] = 'a'\n",
        "\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test.at[i, 'Story'] = test['Source'][i] + ' ' + test['Host'][i] + ' ' + str(test['Link'][i]) + ' ' + test['Title'][i] + ' ' + test['TRANS_CONV_TEXT'][i]\n",
        "\n",
        "test.drop(['Source', 'Host', 'Link', 'Title', 'TRANS_CONV_TEXT', 'Unnamed: 9'], axis = 1, inplace = True)\n",
        "\n",
        "replace_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = replace_space.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = bad_symbols.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text \n",
        "\n",
        "dataset['Story'] = dataset['Story'].apply(preprocess)\n",
        "test['Story'] = test['Story'].apply(preprocess)\n",
        "\n",
        "data_train_list = dataset['Story'].tolist()\n",
        "data_test_list = test['Story'].tolist()\n",
        "\n",
        "all_data = data_train_list + data_test_list\n",
        "\n",
        "\n",
        "\n",
        "tt = WordPunctTokenizer()\n",
        "count_vect=CountVectorizer(tokenizer = tt.tokenize ,stop_words ='english', ngram_range = (1,3))\n",
        "count_vect.fit(all_data)\n",
        "train_countvect = count_vect.transform(dataset.Story)\n",
        "test_countvect = count_vect.transform(test.Story)\n",
        "\n",
        "## Feature Selection\n",
        "#Dimensionality reduction using LSI\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(train_countvect)\n",
        "train_countvect_LSI=svd.transform(train_countvect)\n",
        "\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(test_countvect)\n",
        "test_countvect_LSI=svd.transform(test_countvect)\n",
        "\n",
        "#Feature extraction using TFxIDF\n",
        "tf_idf = TfidfVectorizer(min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1.0,smooth_idf=1.0,sublinear_tf=1.0,\n",
        "            stop_words = 'english')\n",
        "tf_idf.fit(all_data)\n",
        "train_tf= tf_idf.transform(dataset.Story)\n",
        "test_tf = tf_idf.transform(test.Story)\n",
        "\n",
        "## Feature Selection\n",
        "#Dimensionality reduction using LSI\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(train_tf)\n",
        "train_tf_LSI=svd.transform(train_tf)\n",
        "\n",
        "svd=TruncatedSVD(n_components=50,n_iter=10,random_state=42)\n",
        "svd.fit(test_tf)\n",
        "test_tf_LSI=svd.transform(test_tf)\n",
        "\n",
        "# defining parameter range \n",
        "param_grid = {'reg_alpha': np.arange(0,2),\n",
        "              'reg_lambda': np.arange(0,2),\n",
        "     'learning_rate': [0.01, 0.1, 1]\n",
        "             }  \n",
        "cv = KFold(train_tf_LSI.shape[0], shuffle=True, random_state=0)  \n",
        "grid = GridSearchCV(XGBClassifier(), param_grid, cv = cv, n_jobs = -1, refit = True, verbose = 3) \n",
        "\n",
        "# fitting the model for grid search \n",
        "grid.fit(train_tf_LSI, y) \n",
        "\n",
        "# print best parameter after tuning \n",
        "print(grid.best_params_) \n",
        "\n",
        "# print how our model looks after hyper-parameter tuning \n",
        "print(grid.best_estimator_)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 1157 folds for each of 500 candidates, totalling 578500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    6.0s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   18.5s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:   39.4s\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Done 1564 tasks      | elapsed:  3.4min\n",
            "[Parallel(n_jobs=-1)]: Done 2044 tasks      | elapsed:  4.5min\n",
            "[Parallel(n_jobs=-1)]: Done 2588 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done 3196 tasks      | elapsed:  7.0min\n",
            "[Parallel(n_jobs=-1)]: Done 3868 tasks      | elapsed:  8.4min\n",
            "[Parallel(n_jobs=-1)]: Done 4604 tasks      | elapsed: 10.0min\n",
            "[Parallel(n_jobs=-1)]: Done 5404 tasks      | elapsed: 11.7min\n",
            "[Parallel(n_jobs=-1)]: Done 6268 tasks      | elapsed: 13.6min\n",
            "[Parallel(n_jobs=-1)]: Done 7196 tasks      | elapsed: 15.7min\n",
            "[Parallel(n_jobs=-1)]: Done 8188 tasks      | elapsed: 17.9min\n",
            "[Parallel(n_jobs=-1)]: Done 9244 tasks      | elapsed: 20.1min\n",
            "[Parallel(n_jobs=-1)]: Done 10364 tasks      | elapsed: 22.5min\n",
            "[Parallel(n_jobs=-1)]: Done 11548 tasks      | elapsed: 25.0min\n",
            "[Parallel(n_jobs=-1)]: Done 12796 tasks      | elapsed: 27.7min\n",
            "[Parallel(n_jobs=-1)]: Done 14108 tasks      | elapsed: 30.5min\n",
            "[Parallel(n_jobs=-1)]: Done 15484 tasks      | elapsed: 33.4min\n",
            "[Parallel(n_jobs=-1)]: Done 16924 tasks      | elapsed: 36.5min\n",
            "[Parallel(n_jobs=-1)]: Done 18428 tasks      | elapsed: 39.7min\n",
            "[Parallel(n_jobs=-1)]: Done 19996 tasks      | elapsed: 43.0min\n",
            "[Parallel(n_jobs=-1)]: Done 21628 tasks      | elapsed: 46.5min\n",
            "[Parallel(n_jobs=-1)]: Done 23324 tasks      | elapsed: 50.1min\n",
            "[Parallel(n_jobs=-1)]: Done 25084 tasks      | elapsed: 53.8min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDVHwwniuBAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_params = {'reg_alpha': , 'reg_lambda': , 'learning_rate': , 'max_depth': }\n",
        "ada_clf = XBGClassifier(**best_params)\n",
        "ada_clf.fit(train_tf_LSI, y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}